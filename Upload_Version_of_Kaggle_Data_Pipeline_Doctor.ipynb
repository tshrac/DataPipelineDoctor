{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1d93oT7xdkE3Wkkv2uEXa11lp0VduS14Q",
      "authorship_tag": "ABX9TyNvs4/6pI6hxLrnNy2tuNPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Data Pipeline Doctor Agent  \n",
        "### An AI Agent for Detecting, Diagnosing & Fixing Data Pipeline Failures\n",
        "\n",
        "Modern analytics teams depend on complex ETL/ELT pipelines that pull data from APIs, files, and databases into warehouses and dashboards.  \n",
        "When something breaks‚Äîschema changes, null spikes, bad data types, broken transformations‚Äîengineers often only discover it after failed jobs, blown SLAs, or wrong numbers in production reports.\n",
        "\n",
        "In this notebook, I build a **Data Pipeline Doctor Agent**:  \n",
        "an AI-assisted system that combines **rule-based validation** with a **reasoning LLM** to automatically:\n",
        "\n",
        "- Inspect datasets for schema drift, missing columns, null spikes, dtype issues, duplicates  \n",
        "- Parse pipeline logs to detect API failures, SQL errors, timeouts, and stack traces  \n",
        "- Validate downstream transformations (date parsing, divide-by-zero risks, incorrect totals)  \n",
        "- Perform root cause analysis in natural language  \n",
        "- Propose concrete Python and SQL fix strategies  \n",
        "- Output a structured **Pipeline Health Report** (`issues_detected`, `root_causes`, `python_fixes`, `sql_fixes`, `recommended_tests`, `final_status`)\n",
        "\n",
        "Rather than being a single model or notebook trick, this project is designed as a **small diagnostic layer** that could sit on top of a real data platform and act like a ‚Äúvirtual senior data engineer‚Äù doing first-pass triage on pipeline incidents.\n",
        "\n",
        "**Notebook structure**\n",
        "\n",
        "1. **Architecture Overview** ‚Äì high-level design and tools  \n",
        "2. **Synthetic Failure Scenarios** ‚Äì four datasets with realistic pipeline issues  \n",
        "3. **Validation Tools** ‚Äì file, log, and transformation validators  \n",
        "4. **LLM Agent** ‚Äì Gemini-powered reasoning and fix suggestions  \n",
        "5. **Experiments** ‚Äì agent behavior on schema drift, null spikes, dtype mismatch, and transformation errors  \n",
        "6. **Pipeline Health Reports** ‚Äì structured outputs for each scenario  \n",
        "7. **Conclusion & Future Work** ‚Äì how this could evolve into a production-grade assistant for data teams\n",
        "\n"
      ],
      "metadata": {
        "id": "jJ2KL6w406ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "project_path = \"/content/drive/MyDrive/DataPipelineDoctor\"\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "print(\"Folder created:\", project_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeirAdw9IXLh",
        "outputId": "ecc58b45-a8fa-4edf-d6e0-88393e663345",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created: /content/drive/MyDrive/DataPipelineDoctor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Synthetic Broken Data\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Make folder to store synthetic data\n",
        "os.makedirs(\"synthetic_data\", exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1Ô∏è‚É£ DATASET 1: SCHEMA DRIFT\n",
        "# Missing column: 'customer_id'\n",
        "# Extra column: 'unexpected_col'\n",
        "# ==========================================\n",
        "\n",
        "df_schema_drift = pd.DataFrame({\n",
        "    \"order_id\": range(1000, 1100),\n",
        "    \"product\": [\"Item_\" + str(i) for i in range(100)],\n",
        "    \"price\": np.random.uniform(10, 200, 100),\n",
        "    \"unexpected_col\": np.random.choice([\"X\", \"Y\", \"Z\"], 100)\n",
        "})\n",
        "\n",
        "df_schema_drift.to_csv(f\"{project_path}/schema_drift.csv\", index=False)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2Ô∏è‚É£ DATASET 2: NULL SPIKE\n",
        "# Column 'price' has 92% nulls\n",
        "# ==========================================\n",
        "\n",
        "prices = np.random.uniform(50, 500, 100)\n",
        "null_indices = np.random.choice(range(100), size=92, replace=False)\n",
        "prices[null_indices] = None\n",
        "\n",
        "df_null_spike = pd.DataFrame({\n",
        "    \"customer_id\": np.random.randint(1000, 2000, 100),\n",
        "    \"order_id\": range(2000, 2100),\n",
        "    \"price\": prices\n",
        "})\n",
        "\n",
        "df_null_spike.to_csv(f\"{project_path}/null_spike.csv\", index=False)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3Ô∏è‚É£ DATASET 3: DTYPE MISMATCH\n",
        "# Column 'age' contains mixed string + int\n",
        "# ==========================================\n",
        "\n",
        "ages = [random.choice([str(random.randint(18, 70)), random.randint(18, 70)]) for _ in range(100)]\n",
        "\n",
        "df_dtype_mismatch = pd.DataFrame({\n",
        "    \"customer_id\": range(3000, 3100),\n",
        "    \"age\": ages,\n",
        "    \"country\": np.random.choice([\"US\", \"UK\", \"CA\"], 100),\n",
        "})\n",
        "\n",
        "df_dtype_mismatch.to_csv(f\"{project_path}/dtype_mismatch.csv\", index=False)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4Ô∏è‚É£ DATASET 4: TRANSFORMATION ERRORS\n",
        "# - Bad date formats\n",
        "# - Divide-by-zero rows\n",
        "# - Invalid numeric values\n",
        "# ==========================================\n",
        "\n",
        "dates = []\n",
        "for _ in range(100):\n",
        "    if random.random() < 0.2:\n",
        "        dates.append(\"2025/13/45\")  # invalid\n",
        "    else:\n",
        "        dates.append((datetime.today() - timedelta(days=random.randint(1, 1000))).strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "quantity = np.random.randint(0, 10, 100)\n",
        "unit_price = np.random.uniform(5, 50, 100)\n",
        "\n",
        "# Inject divide-by-zero\n",
        "zero_indices = np.random.choice(range(100), size=5, replace=False)\n",
        "quantity[zero_indices] = 0\n",
        "\n",
        "df_transformation_errors = pd.DataFrame({\n",
        "    \"transaction_id\": range(4000, 4100),\n",
        "    \"date\": dates,\n",
        "    \"quantity\": quantity,\n",
        "    \"unit_price\": unit_price,\n",
        "    \"total\": [q * p if q != 0 else None for q, p in zip(quantity, unit_price)]\n",
        "})\n",
        "\n",
        "df_transformation_errors.to_csv(f\"{project_path}/transformation_errors.csv\", index=False)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5Ô∏è‚É£ LOG FILE: mixed API + SQL + Python Errors\n",
        "# ==========================================\n",
        "\n",
        "log_data = \"\"\"\n",
        "[2025-11-16 13:01:22] ERROR: API request failed with status 500 at endpoint /orders/v1\n",
        "Traceback (most recent call last):\n",
        "  File \"pipeline.py\", line 44, in fetch_data\n",
        "    response = requests.get(url, timeout=5)\n",
        "requests.exceptions.Timeout: Request timed out\n",
        "\n",
        "[2025-11-16 13:01:28] WARNING: Column 'customer_id' missing from stage 'silver'\n",
        "[2025-11-16 13:02:10] ERROR: SQL Error: column age cannot be cast to INT\n",
        "[2025-11-16 13:02:45] ERROR: ValueError: time data '2025/13/45' does not match format '%Y-%m-%d'\n",
        "[2025-11-16 13:03:12] ERROR: ZeroDivisionError: division by zero encountered in transformation\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{project_path}/pipeline_logs.txt\", \"w\") as f:\n",
        "    f.write(log_data)\n",
        "\n",
        "\n",
        "print(\"Synthetic datasets and logs generated successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUbXXty4Kfmm",
        "outputId": "a57832b7-8578-4daa-bac0-02b2a15a9073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic datasets and logs generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#File Validator Tools\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Missing Columns\n",
        "# ---------------------------\n",
        "def detect_missing_columns(df, expected_columns):\n",
        "    missing = [col for col in expected_columns if col not in df.columns]\n",
        "    return {\n",
        "        \"check\": \"missing_columns\",\n",
        "        \"expected\": expected_columns,\n",
        "        \"actual\": list(df.columns),\n",
        "        \"missing\": missing,\n",
        "        \"status\": \"fail\" if missing else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Schema Drift\n",
        "# (extra or unexpected columns)\n",
        "# ---------------------------\n",
        "def detect_schema_drift(df, expected_columns):\n",
        "    actual = set(df.columns)\n",
        "    expected = set(expected_columns)\n",
        "\n",
        "    unexpected = list(actual - expected)\n",
        "    missing = list(expected - actual)\n",
        "\n",
        "    return {\n",
        "        \"check\": \"schema_drift\",\n",
        "        \"unexpected_columns\": unexpected,\n",
        "        \"missing_columns\": missing,\n",
        "        \"status\": \"fail\" if unexpected or missing else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Dtype Mismatch\n",
        "# ---------------------------\n",
        "def detect_dtype_mismatch(df, expected_dtypes):\n",
        "    mismatches = {}\n",
        "\n",
        "    for col, expected_type in expected_dtypes.items():\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        actual_type = df[col].dtype\n",
        "\n",
        "        if str(actual_type) != expected_type:\n",
        "            mismatches[col] = {\n",
        "                \"expected\": expected_type,\n",
        "                \"actual\": str(actual_type)\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        \"check\": \"dtype_mismatch\",\n",
        "        \"mismatches\": mismatches,\n",
        "        \"status\": \"fail\" if mismatches else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Null Spike Detection\n",
        "# ---------------------------\n",
        "def detect_null_spike(df, threshold=0.3):  # default 30% null allowed\n",
        "    null_percentages = (df.isnull().mean()).to_dict()\n",
        "    spike_columns = {col: pct for col, pct in null_percentages.items() if pct > threshold}\n",
        "\n",
        "    return {\n",
        "        \"check\": \"null_spike\",\n",
        "        \"null_percentages\": null_percentages,\n",
        "        \"spike_columns\": spike_columns,\n",
        "        \"status\": \"fail\" if spike_columns else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Duplicate Rows\n",
        "# ---------------------------\n",
        "def detect_duplicate_rows(df):\n",
        "    dup_count = df.duplicated().sum()\n",
        "\n",
        "    return {\n",
        "        \"check\": \"duplicate_rows\",\n",
        "        \"duplicate_count\": dup_count,\n",
        "        \"status\": \"fail\" if dup_count > 0 else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"File Validator Tools Loaded Successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRXVF6jRNHYv",
        "outputId": "8b6a6706-4069-429f-9a9d-71447147de08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Validator Tools Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Log Interpreter Tools\n",
        "\n",
        "import re\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1. Extract generic ERROR / WARNING lines\n",
        "# ----------------------------------------\n",
        "def extract_errors(log_text):\n",
        "    pattern = r\"(ERROR|WARNING):.*\"\n",
        "    matches = re.findall(pattern, log_text)\n",
        "    return {\n",
        "        \"check\": \"log_errors\",\n",
        "        \"errors_found\": matches,\n",
        "        \"status\": \"fail\" if matches else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Extract complete stack traces\n",
        "# ----------------------------------------\n",
        "def extract_stacktrace(log_text):\n",
        "    stack_pattern = r\"Traceback([\\s\\S]*?)(?=\\n\\[|\\Z)\"\n",
        "    matches = re.findall(stack_pattern, log_text)\n",
        "    clean_traces = [trace.strip() for trace in matches]\n",
        "\n",
        "    return {\n",
        "        \"check\": \"stacktraces\",\n",
        "        \"stacktraces\": clean_traces,\n",
        "        \"status\": \"fail\" if clean_traces else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. Classify error type (Python / SQL / API)\n",
        "# ----------------------------------------\n",
        "def classify_error_type(log_text):\n",
        "    classifications = []\n",
        "\n",
        "    if \"SQL Error\" in log_text or \"SQL\" in log_text:\n",
        "        classifications.append(\"SQL_Error\")\n",
        "\n",
        "    if \"ValueError\" in log_text or \"TypeError\" in log_text or \"ZeroDivisionError\" in log_text:\n",
        "        classifications.append(\"Python_Error\")\n",
        "\n",
        "    if \"API request failed\" in log_text or \"status 500\" in log_text:\n",
        "        classifications.append(\"API_Error\")\n",
        "\n",
        "    if \"Timeout\" in log_text or \"timed out\" in log_text:\n",
        "        classifications.append(\"Timeout_Error\")\n",
        "\n",
        "    return {\n",
        "        \"check\": \"log_classification\",\n",
        "        \"categories\": classifications,\n",
        "        \"status\": \"fail\" if classifications else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4. Parse API failures specifically\n",
        "# ----------------------------------------\n",
        "def parse_api_failures(log_text):\n",
        "    pattern = r\"API request failed.*\"\n",
        "    api_errors = re.findall(pattern, log_text)\n",
        "\n",
        "    timeout_pattern = r\"timed out\"\n",
        "    timeouts = re.findall(timeout_pattern, log_text)\n",
        "\n",
        "    return {\n",
        "        \"check\": \"api_failures\",\n",
        "        \"api_errors\": api_errors,\n",
        "        \"timeouts\": timeouts,\n",
        "        \"status\": \"fail\" if api_errors or timeouts else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5. Parse SQL failures specifically\n",
        "# ----------------------------------------\n",
        "def parse_sql_failures(log_text):\n",
        "    pattern = r\"SQL Error.*\"\n",
        "    sql_errors = re.findall(pattern, log_text)\n",
        "\n",
        "    return {\n",
        "        \"check\": \"sql_failures\",\n",
        "        \"sql_errors\": sql_errors,\n",
        "        \"status\": \"fail\" if sql_errors else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Log Interpreter Tools Loaded Successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKAMYkUNxiP",
        "outputId": "a8dcee4c-1b3a-4106-c6c3-190588212789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Interpreter Tools Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformation Checker Tools\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1. Test Date Parsing\n",
        "# ----------------------------------------\n",
        "def test_date_parsing(df, date_column, date_format=\"%Y-%m-%d\"):\n",
        "    invalid_dates = []\n",
        "\n",
        "    for idx, val in df[date_column].items():\n",
        "        try:\n",
        "            datetime.strptime(str(val), date_format)\n",
        "        except Exception:\n",
        "            invalid_dates.append({\"row\": idx, \"value\": val})\n",
        "\n",
        "    return {\n",
        "        \"check\": \"date_parsing\",\n",
        "        \"column\": date_column,\n",
        "        \"invalid_dates\": invalid_dates,\n",
        "        \"status\": \"fail\" if len(invalid_dates) > 0 else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Detect Divide-by-Zero Errors\n",
        "# ----------------------------------------\n",
        "def detect_divide_by_zero(df, numerator_col, denominator_col):\n",
        "    zero_indices = df[df[denominator_col] == 0].index.tolist()\n",
        "\n",
        "    return {\n",
        "        \"check\": \"divide_by_zero\",\n",
        "        \"numerator_column\": numerator_col,\n",
        "        \"denominator_column\": denominator_col,\n",
        "        \"zero_rows\": zero_indices,\n",
        "        \"status\": \"fail\" if zero_indices else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. Detect Invalid Numeric Columns\n",
        "# (e.g., strings in numeric fields)\n",
        "# ----------------------------------------\n",
        "def detect_invalid_numeric(df, numeric_columns):\n",
        "    invalid_values = {}\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        invalid = []\n",
        "        for idx, val in df[col].items():\n",
        "            try:\n",
        "                float(val)\n",
        "            except:\n",
        "                invalid.append({\"row\": idx, \"value\": val})\n",
        "        if invalid:\n",
        "            invalid_values[col] = invalid\n",
        "\n",
        "    return {\n",
        "        \"check\": \"invalid_numeric_values\",\n",
        "        \"invalid_values\": invalid_values,\n",
        "        \"status\": \"fail\" if invalid_values else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4. Detect Transformation Failures\n",
        "# (e.g., inconsistent totals, failed calculated fields)\n",
        "# ----------------------------------------\n",
        "def detect_transformation_failures(df, rules):\n",
        "    \"\"\"\n",
        "    rules = {\n",
        "        \"column\": \"total\",\n",
        "        \"logic\": lambda row: row[\"quantity\"] * row[\"unit_price\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    failed_rows = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        expected = rules[\"logic\"](row)\n",
        "        actual = row[rules[\"column\"]]\n",
        "\n",
        "        # consider NaN mismatch\n",
        "        if pd.isna(expected) and pd.isna(actual):\n",
        "            continue\n",
        "\n",
        "        if pd.isna(expected) != pd.isna(actual):\n",
        "            failed_rows.append({\"row\": idx, \"expected\": expected, \"actual\": actual})\n",
        "            continue\n",
        "\n",
        "        if round(expected, 4) != round(actual, 4):\n",
        "            failed_rows.append({\"row\": idx, \"expected\": expected, \"actual\": actual})\n",
        "\n",
        "    return {\n",
        "        \"check\": \"transformation_validation\",\n",
        "        \"column\": rules[\"column\"],\n",
        "        \"failed_rows\": failed_rows,\n",
        "        \"status\": \"fail\" if failed_rows else \"pass\"\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Transformation Checker Tools Loaded Successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2o-pjlLOHfK",
        "outputId": "61dac221-6398-4b15-e792-c045198322c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation Checker Tools Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run_all_validations()\n",
        "\n",
        "import json\n",
        "\n",
        "def run_all_validations(\n",
        "    df=None,\n",
        "    log_text=None,\n",
        "    expected_columns=None,\n",
        "    expected_dtypes=None,\n",
        "    numeric_columns=None,\n",
        "    date_column=None,\n",
        "    date_format=\"%Y-%m-%d\",\n",
        "    numerator_col=None,\n",
        "    denominator_col=None,\n",
        "    transformation_rules=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs ALL validation tools and collects structured diagnostics.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {\n",
        "        \"file_validations\": {},\n",
        "        \"log_validations\": {},\n",
        "        \"transformation_validations\": {},\n",
        "    }\n",
        "\n",
        "    # =================================================\n",
        "    # 1Ô∏è‚É£ File Validations (only if dataset is provided)\n",
        "    # =================================================\n",
        "    if df is not None:\n",
        "        if expected_columns:\n",
        "            results[\"file_validations\"][\"missing_columns\"] = detect_missing_columns(df, expected_columns)\n",
        "\n",
        "        if expected_columns:\n",
        "            results[\"file_validations\"][\"schema_drift\"] = detect_schema_drift(df, expected_columns)\n",
        "\n",
        "        if expected_dtypes:\n",
        "            results[\"file_validations\"][\"dtype_mismatch\"] = detect_dtype_mismatch(df, expected_dtypes)\n",
        "\n",
        "        # Null Spike\n",
        "        results[\"file_validations\"][\"null_spike\"] = detect_null_spike(df)\n",
        "\n",
        "        # Duplicates\n",
        "        results[\"file_validations\"][\"duplicates\"] = detect_duplicate_rows(df)\n",
        "\n",
        "\n",
        "    # =================================================\n",
        "    # 2Ô∏è‚É£ Log Validations (only if logs provided)\n",
        "    # =================================================\n",
        "    if log_text is not None:\n",
        "        results[\"log_validations\"][\"errors\"] = extract_errors(log_text)\n",
        "        results[\"log_validations\"][\"stacktrace\"] = extract_stacktrace(log_text)\n",
        "        results[\"log_validations\"][\"classification\"] = classify_error_type(log_text)\n",
        "        results[\"log_validations\"][\"api_failures\"] = parse_api_failures(log_text)\n",
        "        results[\"log_validations\"][\"sql_failures\"] = parse_sql_failures(log_text)\n",
        "\n",
        "\n",
        "    # =================================================\n",
        "    # 3Ô∏è‚É£ Transformation Validations\n",
        "    # =================================================\n",
        "    if df is not None:\n",
        "\n",
        "        # Date Parsing\n",
        "        if date_column:\n",
        "            results[\"transformation_validations\"][\"date_parsing\"] = test_date_parsing(\n",
        "                df, date_column, date_format\n",
        "            )\n",
        "\n",
        "        # Divide by zero\n",
        "        if numerator_col and denominator_col:\n",
        "            results[\"transformation_validations\"][\"divide_by_zero\"] = detect_divide_by_zero(\n",
        "                df, numerator_col, denominator_col\n",
        "            )\n",
        "\n",
        "        # Invalid numeric values\n",
        "        if numeric_columns:\n",
        "            results[\"transformation_validations\"][\"invalid_numeric\"] = detect_invalid_numeric(\n",
        "                df, numeric_columns\n",
        "            )\n",
        "\n",
        "        # Transform logic validation\n",
        "        if transformation_rules:\n",
        "            results[\"transformation_validations\"][\"failed_transforms\"] = detect_transformation_failures(\n",
        "                df, transformation_rules\n",
        "            )\n",
        "\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "QUOc1zEcPfeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Gemini GenAI"
      ],
      "metadata": {
        "id": "qyJlZAah2F3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-generativeai -q"
      ],
      "metadata": {
        "id": "YJhiWNO9cS5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your key securely\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Gemini API key: \")\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "print(\"Configured Gemini API.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBcjULhsi0OC",
        "outputId": "d094c1d5-6e5c-42f8-a49a-601f0f825bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Gemini API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Configured Gemini API.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# We already called genai.configure(...) in the previous cell\n",
        "\n",
        "MODEL_NAME = \"gemini-2.0-flash\"   # current, supported model\n",
        "\n",
        "model = genai.GenerativeModel(MODEL_NAME)\n",
        "\n",
        "response = model.generate_content(\"Say exactly: Gemini is working now.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "5hT-yOaojYUe",
        "outputId": "264f8740-b7fa-4762-db7c-6c4e6a9b1f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini is working now.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = genai.list_models()\n",
        "for m in models:\n",
        "    if \"generateContent\" in getattr(m, \"supported_generation_methods\", []):\n",
        "        print(m.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "collapsed": true,
        "id": "BUQApReHjawr",
        "outputId": "ab7f392d-1bf0-4292-e9aa-0cb7a62d9b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1#Ô∏è‚É£ Make sure Gemini model is set (using the working one)\n",
        "#Use the same model name that just worked :\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# assumes you've already set os.environ[\"GOOGLE_API_KEY\"]\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "MODEL_NAME = \"gemini-2.0-flash\"   # or whatever you just used successfully\n",
        "model = genai.GenerativeModel(MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "2JJ0-SqMkXZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Re-define the agent reasoning function (Gemini-based)\n",
        "#This takes the diagnostics from your tools and asks Gemini to act like a senior data engineer:\n",
        "def agent_reasoning_gemini(diagnostics):\n",
        "    prompt = f\"\"\"\n",
        "You are the **Data Pipeline Doctor Agent**, an expert senior Data Engineer.\n",
        "\n",
        "Below are validation results collected from a broken data pipeline.\n",
        "Analyze them and produce:\n",
        "\n",
        "1. A concise list of all issues detected.\n",
        "2. Root cause analysis (why it happened in ETL/ELT context).\n",
        "3. Python code suggestions to fix the data / pipeline.\n",
        "4. SQL code suggestions where relevant.\n",
        "5. A final Pipeline Health Report in this JSON-like structure:\n",
        "\n",
        "\n",
        "{{\n",
        " \"issues_detected\": [...],\n",
        " \"root_causes\": [...],\n",
        " \"python_fixes\": [...],\n",
        " \"sql_fixes\": [...],\n",
        " \"recommended_tests\": [...],\n",
        " \"final_status\": \"healthy\" | \"unhealthy\"\n",
        " }}\n",
        "\n",
        "Diagnostics:\n",
        "{diagnostics}\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "N-ibULL4lOM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3Ô∏è‚É£ Load the schema_drift dataset + logs\n",
        "#Adjust this depending on where you saved the files.\n",
        "#üîπ Option A ‚Äî You saved in Google Drive (recommended)\n",
        "import pandas as pd\n",
        "\n",
        "# change this to your actual folder in Drive\n",
        "project_path = \"/content/drive/MyDrive/DataPipelineDoctor\"\n",
        "\n",
        "df_schema = pd.read_csv(f\"{project_path}/schema_drift.csv\")\n",
        "with open(f\"{project_path}/pipeline_logs.txt\", \"r\") as f:\n",
        "    log_text = f.read()\n"
      ],
      "metadata": {
        "id": "ACTy5uJRlr9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run all validations on this dataset\n",
        "#For the schema drift example, let‚Äôs say we expect a clean orders table with:\n",
        "# order_id, customer_id, product, price\n",
        "\n",
        "# But our file actually has:\n",
        "# order_id, product, price, unexpected_col\n",
        "\n",
        "# Here‚Äôs the validation call:\n",
        "expected_columns = [\"order_id\", \"customer_id\", \"product\", \"price\"]\n",
        "expected_dtypes = {\n",
        "    \"order_id\": \"int64\",\n",
        "    \"customer_id\": \"int64\",     # will be missing\n",
        "    \"product\": \"object\",\n",
        "    \"price\": \"float64\"\n",
        "}\n",
        "\n",
        "diagnostics = run_all_validations(\n",
        "    df = df_schema,\n",
        "    log_text = log_text,\n",
        "    expected_columns = expected_columns,\n",
        "    expected_dtypes = expected_dtypes,\n",
        "    numeric_columns = [\"price\"],      # for null/invalid numeric checks\n",
        "    # no date / transform rules for this first demo\n",
        ")\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(diagnostics)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rhgFIucNlzYU",
        "outputId": "ea7db953-1e70-4671-8f55-5b85083d0f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_validations': {'dtype_mismatch': {'check': 'dtype_mismatch',\n",
            "                                         'mismatches': {},\n",
            "                                         'status': 'pass'},\n",
            "                      'duplicates': {'check': 'duplicate_rows',\n",
            "                                     'duplicate_count': np.int64(0),\n",
            "                                     'status': 'pass'},\n",
            "                      'missing_columns': {'actual': ['order_id',\n",
            "                                                     'product',\n",
            "                                                     'price',\n",
            "                                                     'unexpected_col'],\n",
            "                                          'check': 'missing_columns',\n",
            "                                          'expected': ['order_id',\n",
            "                                                       'customer_id',\n",
            "                                                       'product',\n",
            "                                                       'price'],\n",
            "                                          'missing': ['customer_id'],\n",
            "                                          'status': 'fail'},\n",
            "                      'null_spike': {'check': 'null_spike',\n",
            "                                     'null_percentages': {'order_id': 0.0,\n",
            "                                                          'price': 0.0,\n",
            "                                                          'product': 0.0,\n",
            "                                                          'unexpected_col': 0.0},\n",
            "                                     'spike_columns': {},\n",
            "                                     'status': 'pass'},\n",
            "                      'schema_drift': {'check': 'schema_drift',\n",
            "                                       'missing_columns': ['customer_id'],\n",
            "                                       'status': 'fail',\n",
            "                                       'unexpected_columns': ['unexpected_col']}},\n",
            " 'log_validations': {'api_failures': {'api_errors': ['API request failed with '\n",
            "                                                     'status 500 at endpoint '\n",
            "                                                     '/orders/v1'],\n",
            "                                      'check': 'api_failures',\n",
            "                                      'status': 'fail',\n",
            "                                      'timeouts': ['timed out']},\n",
            "                     'classification': {'categories': ['SQL_Error',\n",
            "                                                       'Python_Error',\n",
            "                                                       'API_Error',\n",
            "                                                       'Timeout_Error'],\n",
            "                                        'check': 'log_classification',\n",
            "                                        'status': 'fail'},\n",
            "                     'errors': {'check': 'log_errors',\n",
            "                                'errors_found': ['ERROR',\n",
            "                                                 'WARNING',\n",
            "                                                 'ERROR',\n",
            "                                                 'ERROR',\n",
            "                                                 'ERROR'],\n",
            "                                'status': 'fail'},\n",
            "                     'sql_failures': {'check': 'sql_failures',\n",
            "                                      'sql_errors': ['SQL Error: column age '\n",
            "                                                     'cannot be cast to INT'],\n",
            "                                      'status': 'fail'},\n",
            "                     'stacktrace': {'check': 'stacktraces',\n",
            "                                    'stacktraces': ['(most recent call last):\\n'\n",
            "                                                    '  File \"pipeline.py\", '\n",
            "                                                    'line 44, in fetch_data\\n'\n",
            "                                                    '    response = '\n",
            "                                                    'requests.get(url, '\n",
            "                                                    'timeout=5)\\n'\n",
            "                                                    'requests.exceptions.Timeout: '\n",
            "                                                    'Request timed out'],\n",
            "                                    'status': 'fail'}},\n",
            " 'transformation_validations': {'invalid_numeric': {'check': 'invalid_numeric_values',\n",
            "                                                    'invalid_values': {},\n",
            "                                                    'status': 'pass'}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5Let Gemini act as the Data Pipeline Doctor ü©∫\n",
        "#Now call the agent:\n",
        "report = agent_reasoning_gemini(diagnostics)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AFRWd3UenOhc",
        "outputId": "fbef1997-4b5b-4b1e-9fb8-9a494529df0d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I've reviewed the validation results. This pipeline has several issues. Let's break down the problems, their causes, and potential solutions.\n",
            "\n",
            "**1. Issues Detected:**\n",
            "\n",
            "*   **Missing Column:** The 'customer\\_id' column is missing from the file being processed.\n",
            "*   **Unexpected Column:** An 'unexpected\\_col' column is present in the file.\n",
            "*   **Log Errors:** The logs contain errors and warnings.\n",
            "*   **Stacktraces:** Stacktraces are present in the logs, indicating code execution problems.\n",
            "*   **Log Classification:** Logs indicate SQL, Python, API, and Timeout errors.\n",
            "*   **API Failures:** API requests are failing with a 500 status code and timeouts.\n",
            "*   **SQL Failures:** SQL errors are occurring due to type conversion issues.\n",
            "\n",
            "**2. Root Cause Analysis:**\n",
            "\n",
            "*   **Missing/Unexpected Column (Schema Drift):** This suggests a change in the structure of the input file or data source. This could be due to:\n",
            "    *   Upstream data source schema changes (e.g., a new version of an API or database table).\n",
            "    *   Errors in the data extraction or transformation logic that accidentally dropped or added columns.\n",
            "    *   A change in the file format or source.\n",
            "*   **Log Errors, Stacktraces, API Failures, Timeout Errors:** These issues indicate problems in the pipeline's code and its interaction with external services. Likely causes:\n",
            "    *   **API Errors:** Issues with the API endpoint `/orders/v1`, such as server errors, rate limiting, or incorrect request formatting.\n",
            "    *   **Timeout Errors:** Network connectivity problems, overloaded API servers, or inefficient queries causing long processing times. The stacktrace points to a timeout in the `requests.get` call.\n",
            "    *   **SQL Errors:** Incompatible data types being inserted into the database. The specific error indicates a failure to convert a 'column age' to an integer, implying that column contains non-numeric data.\n",
            "*   **Log Classification:** Further clarifies the origin of the problems.\n",
            "\n",
            "**3. Python Code Suggestions:**\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "import requests\n",
            "\n",
            "# --- Handling Schema Drift ---\n",
            "def handle_schema_drift(df, expected_columns):\n",
            "    \"\"\"\n",
            "    Corrects schema drift by adding missing columns and removing unexpected ones.\n",
            "\n",
            "    Args:\n",
            "        df (pd.DataFrame): The DataFrame to clean.\n",
            "        expected_columns (list): A list of expected column names.\n",
            "\n",
            "    Returns:\n",
            "        pd.DataFrame: The cleaned DataFrame.\n",
            "    \"\"\"\n",
            "    actual_columns = df.columns.tolist()\n",
            "    missing_columns = set(expected_columns) - set(actual_columns)\n",
            "    unexpected_columns = set(actual_columns) - set(expected_columns)\n",
            "\n",
            "    for col in missing_columns:\n",
            "        df[col] = None  # Or a more appropriate default value\n",
            "\n",
            "    df = df[expected_columns]  # Select only the expected columns\n",
            "\n",
            "    return df\n",
            "\n",
            "\n",
            "# Example usage (assuming you have a DataFrame called 'data'):\n",
            "# expected_columns = ['order_id', 'customer_id', 'product', 'price']\n",
            "# data = handle_schema_drift(data, expected_columns)\n",
            "\n",
            "\n",
            "# --- Retry Mechanism for API Calls ---\n",
            "import time\n",
            "def fetch_data_with_retry(url, max_retries=3, backoff_factor=2):\n",
            "    \"\"\"Fetches data from a URL with retry logic.\"\"\"\n",
            "    for attempt in range(max_retries):\n",
            "        try:\n",
            "            response = requests.get(url, timeout=5)\n",
            "            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
            "            return response\n",
            "        except requests.exceptions.RequestException as e:\n",
            "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
            "            if attempt < max_retries - 1:\n",
            "                sleep_time = backoff_factor ** attempt\n",
            "                print(f\"Retrying in {sleep_time} seconds...\")\n",
            "                time.sleep(sleep_time)\n",
            "    print(f\"Failed to fetch data after {max_retries} attempts.\")\n",
            "    return None  # Or raise an exception\n",
            "# Example Usage\n",
            "#url = 'your_api_endpoint'\n",
            "#response = fetch_data_with_retry(url)\n",
            "#if response:\n",
            "#    data = response.json()\n",
            "#    # Process the data\n",
            "\n",
            "# --- Error Logging Improvement (Example) ---\n",
            "import logging\n",
            "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
            "\n",
            "# Within your pipeline code:\n",
            "# try:\n",
            "#     # ... your code that might fail ...\n",
            "# except Exception as e:\n",
            "#     logging.error(f\"An error occurred: {e}\", exc_info=True)  # Log the full stacktrace\n",
            "\n",
            "```\n",
            "\n",
            "**4. SQL Code Suggestions:**\n",
            "\n",
            "```sql\n",
            "-- Data Type Conversion Handling (for 'age' column)\n",
            "-- Before inserting into the target table:\n",
            "\n",
            "--Option 1: Filter out rows with invalid age values\n",
            "SELECT *\n",
            "FROM source_table\n",
            "WHERE TRY_CAST(age AS INT) IS NOT NULL;\n",
            "\n",
            "--Option 2: Provide a default value for invalid age values\n",
            "SELECT\n",
            "    CASE\n",
            "        WHEN TRY_CAST(age AS INT) IS NOT NULL THEN CAST(age AS INT)\n",
            "        ELSE -1  -- Or another appropriate default\n",
            "    END AS age,\n",
            "    other_columns\n",
            "FROM source_table;\n",
            "\n",
            "--Option 3: Cleanse the data before attempting to cast.\n",
            "SELECT\n",
            "    CASE\n",
            "        WHEN REGEXP_LIKE(age, '^[0-9]+$') THEN CAST(age AS INT) --Check if the age contains only numerical chars\n",
            "        ELSE -1  -- Or another appropriate default\n",
            "    END AS age,\n",
            "    other_columns\n",
            "FROM source_table;\n",
            "\n",
            "-- Option 4: Create a Staging Table with Text Column\n",
            "-- 1. Create a staging table with the 'age' column as TEXT.\n",
            "CREATE TABLE staging_table (\n",
            "    order_id VARCHAR(255),\n",
            "    customer_id VARCHAR(255),\n",
            "    product VARCHAR(255),\n",
            "    price DECIMAL(10, 2),\n",
            "    age TEXT  -- Important: Age is TEXT\n",
            ");\n",
            "\n",
            "-- 2. Load the data into the staging table.\n",
            "\n",
            "-- 3. Perform data cleansing and type conversion in SQL:\n",
            "INSERT INTO final_table (order_id, customer_id, product, price, age)\n",
            "SELECT order_id, customer_id, product, price,\n",
            "       CASE\n",
            "           WHEN REGEXP_LIKE(age, '^[0-9]+$') THEN CAST(age AS INT)\n",
            "           ELSE NULL  -- Or a default value\n",
            "       END\n",
            "FROM staging_table;\n",
            "```\n",
            "\n",
            "**5. Recommended Tests:**\n",
            "\n",
            "*   **Schema Validation:** Implement tests to verify the presence and data types of required columns *before* processing each file.  Fail fast if the schema is incorrect.\n",
            "*   **Data Type Validation:** Create tests to ensure data conforms to expected data types (e.g., price is numeric, age is an integer or can be cast to an integer).\n",
            "*   **API Response Validation:**  Test the API endpoints proactively to verify they return expected responses (status codes, data format).\n",
            "*   **End-to-End Integration Tests:** Run tests to verify that data flows correctly through the entire pipeline and that data in the target database matches expectations.\n",
            "*   **Timeout Monitoring:** Monitor API and SQL query execution times to detect performance regressions or potential timeout issues.\n",
            "\n",
            "**6. Final Pipeline Health Report:**\n",
            "\n",
            "```json\n",
            "{\n",
            " \"issues_detected\": [\n",
            "  \"Missing column 'customer_id'\",\n",
            "  \"Unexpected column 'unexpected_col'\",\n",
            "  \"Log errors present\",\n",
            "  \"Stacktraces detected\",\n",
            "  \"API failures with status 500 at /orders/v1\",\n",
            "  \"Timeout errors in API requests\",\n",
            "  \"SQL errors due to data type conversion failure (age to INT)\"\n",
            " ],\n",
            " \"root_causes\": [\n",
            "  \"Upstream data source schema changes (missing/unexpected columns)\",\n",
            "  \"API server errors or rate limiting\",\n",
            "  \"Network connectivity problems causing timeouts\",\n",
            "  \"Invalid data in 'age' column preventing type conversion to INT\"\n",
            " ],\n",
            " \"python_fixes\": [\n",
            "  \"Implement schema validation and correction using `handle_schema_drift` function.\",\n",
            "  \"Implement retry mechanism for API calls using `fetch_data_with_retry` function.\",\n",
            "  \"Improve error logging with stacktrace information.\",\n",
            "  \"Implement checks to ensure only valid customer IDs are written into the pipeline.\"\n",
            " ],\n",
            " \"sql_fixes\": [\n",
            "  \"Filter out or cleanse invalid 'age' values before inserting into the target table.\",\n",
            "  \"Use TRY_CAST to handle potential conversion errors gracefully.\",\n",
            "  \"Consider creating a staging table with TEXT columns for initial data loading and cleansing.\"\n",
            " ],\n",
            " \"recommended_tests\": [\n",
            "  \"Schema validation tests\",\n",
            "  \"Data type validation tests\",\n",
            "  \"API response validation tests\",\n",
            "  \"End-to-end integration tests\",\n",
            "  \"Timeout monitoring\",\n",
            "  \"Add tests for customer_id to determine type, format, and validity.\"\n",
            " ],\n",
            " \"final_status\": \"unhealthy\"\n",
            "}\n",
            "```\n",
            "\n",
            "**Explanation of the Report:**\n",
            "\n",
            "*   **`issues_detected`:** A clear summary of all the problems found.\n",
            "*   **`root_causes`:** The most likely reasons for those problems.\n",
            "*   **`python_fixes`:** Python code snippets (with explanations) to address some of the issues.  This includes schema handling, API retries, and better error logging.\n",
            "*   **`sql_fixes`:** SQL code to handle data type conversion errors and potential data cleansing.\n",
            "*   **`recommended_tests`:** Suggestions for adding automated tests to prevent regressions and improve pipeline reliability.\n",
            "*   **`final_status`:**  Overall health assessment.  In this case, the pipeline is clearly \"unhealthy\" and requires immediate attention.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the JSON block for programmatic use\n",
        "# Right now the model returns text with a JSON-looking block at the end.\n",
        "#  We can grab that block and parse it so we can:\n",
        "# display status nicely\n",
        "# use it in a dashboard / tablencompare multiple runs\n",
        "\n",
        "import re, json\n",
        "\n",
        "def extract_health_json(report_text):\n",
        "    \"\"\"\n",
        "    Extract the JSON-like Pipeline Health Report from the LLM text.\n",
        "    Returns a Python dict or None if parsing fails.\n",
        "    \"\"\"\n",
        "    # 1) Try to find a ```json ... ``` fenced block\n",
        "    pattern = r\"```json\\s*({.*?})\\s*```\"\n",
        "    match = re.search(pattern, report_text, re.S)\n",
        "\n",
        "    if not match:\n",
        "        print(\"No ```json fenced block found in report.\")\n",
        "        return None\n",
        "\n",
        "    json_str = match.group(1)\n",
        "\n",
        "    try:\n",
        "        health = json.loads(json_str)\n",
        "        return health\n",
        "    except Exception as e:\n",
        "        print(\"Failed to parse JSON from report:\", e)\n",
        "        print(\"\\nRaw JSON string preview:\\n\", json_str[:500])\n",
        "        return None"
      ],
      "metadata": {
        "id": "ZAB7IIW8qC2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health = extract_health_json(report)\n",
        "health"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-124azPsqaDa",
        "outputId": "aee4239c-659c-401f-9146-eabab3e5e328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'issues_detected': [\"Missing column 'customer_id'\",\n",
              "  \"Unexpected column 'unexpected_col'\",\n",
              "  'Log errors present',\n",
              "  'Stacktraces detected',\n",
              "  'API failures with status 500 at /orders/v1',\n",
              "  'Timeout errors in API requests',\n",
              "  'SQL errors due to data type conversion failure (age to INT)'],\n",
              " 'root_causes': ['Upstream data source schema changes (missing/unexpected columns)',\n",
              "  'API server errors or rate limiting',\n",
              "  'Network connectivity problems causing timeouts',\n",
              "  \"Invalid data in 'age' column preventing type conversion to INT\"],\n",
              " 'python_fixes': ['Implement schema validation and correction using `handle_schema_drift` function.',\n",
              "  'Implement retry mechanism for API calls using `fetch_data_with_retry` function.',\n",
              "  'Improve error logging with stacktrace information.',\n",
              "  'Implement checks to ensure only valid customer IDs are written into the pipeline.'],\n",
              " 'sql_fixes': [\"Filter out or cleanse invalid 'age' values before inserting into the target table.\",\n",
              "  'Use TRY_CAST to handle potential conversion errors gracefully.',\n",
              "  'Consider creating a staging table with TEXT columns for initial data loading and cleansing.'],\n",
              " 'recommended_tests': ['Schema validation tests',\n",
              "  'Data type validation tests',\n",
              "  'API response validation tests',\n",
              "  'End-to-end integration tests',\n",
              "  'Timeout monitoring',\n",
              "  'Add tests for customer_id to determine type, format, and validity.'],\n",
              " 'final_status': 'unhealthy'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Pretty-print the Pipeline Health Report\n",
        "# Add this cell:\n",
        "def display_health_report(health):\n",
        "    if health is None:\n",
        "        print(\"No structured health report available.\")\n",
        "        return\n",
        "\n",
        "    print(\"=== PIPELINE HEALTH REPORT ===\\n\")\n",
        "    print(\"Final Status:\", health.get(\"final_status\", \"unknown\").upper())\n",
        "\n",
        "    print(\"\\nIssues Detected:\")\n",
        "    for issue in health.get(\"issues_detected\", []):\n",
        "        print(\"  -\", issue)\n",
        "\n",
        "    print(\"\\nRoot Causes:\")\n",
        "    for rc in health.get(\"root_causes\", []):\n",
        "        print(\"  -\", rc)\n",
        "\n",
        "    print(\"\\nPython Fixes:\")\n",
        "    for fix in health.get(\"python_fixes\", []):\n",
        "        print(\"  -\", fix)\n",
        "\n",
        "    print(\"\\nSQL Fixes:\")\n",
        "    for fix in health.get(\"sql_fixes\", []):\n",
        "        print(\"  -\", fix)\n",
        "\n",
        "    print(\"\\nRecommended Tests:\")\n",
        "    for test in health.get(\"recommended_tests\", []):\n",
        "        print(\"  -\", test)\n"
      ],
      "metadata": {
        "id": "sKhAms7EsVmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_health_report(health)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "me7SKk50sX_t",
        "outputId": "fd8b90a2-46ce-4cd9-eef3-b5c56d349acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PIPELINE HEALTH REPORT ===\n",
            "\n",
            "Final Status: UNHEALTHY\n",
            "\n",
            "Issues Detected:\n",
            "  - Missing column 'customer_id'\n",
            "  - Unexpected column 'unexpected_col'\n",
            "  - Log errors present\n",
            "  - Stacktraces detected\n",
            "  - API failures with status 500 at /orders/v1\n",
            "  - Timeout errors in API requests\n",
            "  - SQL errors due to data type conversion failure (age to INT)\n",
            "\n",
            "Root Causes:\n",
            "  - Upstream data source schema changes (missing/unexpected columns)\n",
            "  - API server errors or rate limiting\n",
            "  - Network connectivity problems causing timeouts\n",
            "  - Invalid data in 'age' column preventing type conversion to INT\n",
            "\n",
            "Python Fixes:\n",
            "  - Implement schema validation and correction using `handle_schema_drift` function.\n",
            "  - Implement retry mechanism for API calls using `fetch_data_with_retry` function.\n",
            "  - Improve error logging with stacktrace information.\n",
            "  - Implement checks to ensure only valid customer IDs are written into the pipeline.\n",
            "\n",
            "SQL Fixes:\n",
            "  - Filter out or cleanse invalid 'age' values before inserting into the target table.\n",
            "  - Use TRY_CAST to handle potential conversion errors gracefully.\n",
            "  - Consider creating a staging table with TEXT columns for initial data loading and cleansing.\n",
            "\n",
            "Recommended Tests:\n",
            "  - Schema validation tests\n",
            "  - Data type validation tests\n",
            "  - API response validation tests\n",
            "  - End-to-end integration tests\n",
            "  - Timeout monitoring\n",
            "  - Add tests for customer_id to determine type, format, and validity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap everything into a single diagnose_pipeline() helper\n",
        "# This will make your demos super simple and reusable.\n",
        "def diagnose_pipeline(\n",
        "    df,\n",
        "    log_text,\n",
        "    expected_columns,\n",
        "    expected_dtypes,\n",
        "    numeric_columns=None,\n",
        "    date_column=None,\n",
        "    date_format=\"%Y-%m-%d\",\n",
        "    numerator_col=None,\n",
        "    denominator_col=None,\n",
        "    transformation_rules=None,\n",
        "):\n",
        "    # 1) run all our validator tools\n",
        "    diagnostics = run_all_validations(\n",
        "        df=df,\n",
        "        log_text=log_text,\n",
        "        expected_columns=expected_columns,\n",
        "        expected_dtypes=expected_dtypes,\n",
        "        numeric_columns=numeric_columns,\n",
        "        date_column=date_column,\n",
        "        date_format=date_format,\n",
        "        numerator_col=numerator_col,\n",
        "        denominator_col=denominator_col,\n",
        "        transformation_rules=transformation_rules,\n",
        "    )\n",
        "\n",
        "    # 2) ask Gemini to act as the Data Pipeline Doctor\n",
        "    report = agent_reasoning_gemini(diagnostics)\n",
        "\n",
        "    # 3) parse JSON-like block + display nicely\n",
        "    health = extract_health_json(report)\n",
        "\n",
        "    print(\"=== RAW LLM REPORT (preview) ===\\n\")\n",
        "    print(report[:800], \"...\\n\")  # just a preview\n",
        "\n",
        "    print(\"=== STRUCTURED HEALTH REPORT ===\\n\")\n",
        "    display_health_report(health)\n",
        "\n",
        "    return diagnostics, report, health\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nc8RxVl8sy5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß™ Demo 1 ‚Äî Schema Drift & Missing Columns\n",
        "\n",
        "**Scenario**\n",
        "\n",
        "An upstream source changed the schema of an orders file:\n",
        "\n",
        "- The expected schema is: `order_id, customer_id, product, price`\n",
        "- The actual file has: `order_id, product, price, unexpected_col`\n",
        "- At the same time, the pipeline logs contain API timeouts and SQL cast errors\n",
        "\n",
        "This is a classic **schema drift** situation: downstream code still assumes the old schema, while the source has changed.\n",
        "\n",
        "**What the agent does**\n",
        "\n",
        "1. The **File Validator** detects:\n",
        "   - Missing column: `customer_id`\n",
        "   - Unexpected column: `unexpected_col`\n",
        "2. The **Log Interpreter** finds:\n",
        "   - API 500 and timeout errors on `/orders/v1`\n",
        "   - SQL errors when casting `age` to INT\n",
        "   - Python stack traces\n",
        "3. All diagnostics are passed to the **LLM agent**, which:\n",
        "   - Summarizes all issues (schema + API + SQL)\n",
        "   - Explains likely root causes (upstream change, unclean data, fragile API integration)\n",
        "   - Suggests Python fixes (schema alignment, retry logic, better logging)\n",
        "   - Suggests SQL fixes (TRY_CAST, staging tables, cleansing flows)\n",
        "   - Outputs a structured `Pipeline Health Report` with `final_status = \"unhealthy\"`\n",
        "\n",
        "This demo shows how the agent connects **schema-level issues** with **log-level failures**, instead of treating them as separate, unrelated problems.\n"
      ],
      "metadata": {
        "id": "AKF-Xc2S3BCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now your schema drift demo can be as simple as:\n",
        "# # adjust paths according to your setup\n",
        "project_path = \"/content/drive/MyDrive/DataPipelineDoctor\"\n",
        "\n",
        "df_schema = pd.read_csv(f\"{project_path}/schema_drift.csv\")\n",
        "with open(f\"{project_path}/pipeline_logs.txt\") as f:\n",
        "    log_text = f.read()\n",
        "\n",
        "diagnostics, report, health = diagnose_pipeline(\n",
        "    df=df_schema,\n",
        "    log_text=log_text,\n",
        "    expected_columns=[\"order_id\", \"customer_id\", \"product\", \"price\"],\n",
        "    expected_dtypes={\n",
        "        \"order_id\": \"int64\",\n",
        "        \"customer_id\": \"int64\",\n",
        "        \"product\": \"object\",\n",
        "        \"price\": \"float64\",\n",
        "    },\n",
        "    numeric_columns=[\"price\"],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "HI-A9vJwtACP",
        "outputId": "fb8bf701-cd7f-462a-8a7a-e0e86c1201d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW LLM REPORT (preview) ===\n",
            "\n",
            "Okay, I've reviewed the validation results. It seems like we have several issues plaguing our data pipeline.  Let's break it down.\n",
            "\n",
            "Here's my analysis:\n",
            "\n",
            "**1. Issues Detected:**\n",
            "\n",
            "*   **Missing Column:** The `customer_id` column is missing from the input data file.\n",
            "*   **Schema Drift:** An unexpected column `unexpected_col` is present in the data file.\n",
            "*   **Log Errors:** The logs contain errors and warnings.\n",
            "*   **Stacktrace:** Stacktraces are present in the logs, indicating exceptions.\n",
            "*   **Log Classification:** Logs show various error categories (SQL, Python, API, Timeout).\n",
            "*   **API Failures:** API requests are failing with 500 errors and timeouts.\n",
            "*   **SQL Failures:** SQL errors are occurring due to type conversion issues.\n",
            "\n",
            "**2. Root Cause Analysis:**\n",
            "\n",
            "*   **Missing Column & Schema Dr ...\n",
            "\n",
            "=== STRUCTURED HEALTH REPORT ===\n",
            "\n",
            "=== PIPELINE HEALTH REPORT ===\n",
            "\n",
            "Final Status: UNHEALTHY\n",
            "\n",
            "Issues Detected:\n",
            "  - Missing column: customer_id\n",
            "  - Schema drift: unexpected column 'unexpected_col'\n",
            "  - Log errors and warnings\n",
            "  - Stacktraces present in logs\n",
            "  - API failures (500 errors, timeouts)\n",
            "  - SQL failures (type conversion error)\n",
            "\n",
            "Root Causes:\n",
            "  - Upstream data source schema changes (missing customer_id, unexpected_col)\n",
            "  - API endpoint instability (500 errors, timeouts)\n",
            "  - Data type incompatibility between source data and target database (age column)\n",
            "  - Potential issues with data fetching/processing logic\n",
            "\n",
            "Python Fixes:\n",
            "  - Implemented error handling in fetch_data function to catch API exceptions and HTTP errors.\n",
            "  - Added logic to process_data function to handle missing columns by adding them with default values.\n",
            "  - Added logic to process_data function to handle schema drift by dropping unexpected columns.\n",
            "  - Implemented data type validation and conversion for the price column.\n",
            "  - Increased the request timeout to 10 seconds to mitigate timeout errors.\n",
            "  - Added comprehensive logging to track errors, warnings, and important events during the pipeline execution.\n",
            "\n",
            "SQL Fixes:\n",
            "  - Used ALTER TABLE with a CASE statement to handle the type conversion of the age column, defaulting to 0 for invalid values.\n",
            "\n",
            "Recommended Tests:\n",
            "  - Schema validation tests (missing/unexpected columns)\n",
            "  - Data type validation tests\n",
            "  - API availability monitoring\n",
            "  - Error rate monitoring\n",
            "  - Data completeness checks\n",
            "  - Data quality checks\n",
            "  - End-to-end tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß™ Demo 2 ‚Äî Null Spike in a Critical Column\n",
        "\n",
        "**Scenario**\n",
        "\n",
        "Here the schema is correct, but the data quality is not:\n",
        "\n",
        "- Table: `customer_id, order_id, price`\n",
        "- Column `price` is **~92% null**, which makes metrics like revenue, AOV, and margin completely unreliable.\n",
        "\n",
        "This kind of issue is subtle: pipelines do not crash, but dashboards become meaningless.\n",
        "\n",
        "**What the agent does**\n",
        "\n",
        "1. The **Null Spike detector** computes null percentages per column and flags:\n",
        "   - `price` as a critical spike column\n",
        "2. Other validators (schema, dtype, duplicates) run as usual\n",
        "3. The **LLM agent**:\n",
        "   - Recognizes that `price` is a business-critical field\n",
        "   - Explains possible root causes (upstream system outage, device bug, bad joins, partial ingestion)\n",
        "   - Suggests Python fixes:\n",
        "     - Fast checks to fail fast when null % exceeds a threshold\n",
        "     - Imputation strategies (e.g., median/mean, or requiring re-ingestion)\n",
        "   - Suggests SQL fixes:\n",
        "     - Filters to exclude null `price` from downstream aggregates\n",
        "     - Data quality checks / assertions using COUNT and NULL ratios\n",
        "   - Marks the pipeline `unhealthy` and recommends additional tests/alerts\n",
        "\n",
        "This demo shows that the agent is not only about ‚Äúhard‚Äù failures (crashes), but also about **silent data quality degradation**.\n"
      ],
      "metadata": {
        "id": "Ef10G1IL3EKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Null Spike in Critical Column\n",
        "import pandas as pd\n",
        "\n",
        "# ==== Choose ONE of these paths ====\n",
        "# If using Google Drive:\n",
        "project_path = \"/content/drive/MyDrive/DataPipelineDoctor\"\n",
        "df_null = pd.read_csv(f\"{project_path}/null_spike.csv\")\n",
        "with open(f\"{project_path}/pipeline_logs.txt\") as f:\n",
        "    log_text = f.read()\n",
        "\n",
        "# If using local Colab folder instead, use this instead:\n",
        "# df_null = pd.read_csv(\"synthetic_data/null_spike.csv\")\n",
        "# with open(\"synthetic_data/pipeline_logs.txt\") as f:\n",
        "#     log_text = f.read()\n",
        "\n",
        "diagnostics_null, report_null, health_null = diagnose_pipeline(\n",
        "    df=df_null,\n",
        "    log_text=log_text,\n",
        "    expected_columns=[\"customer_id\", \"order_id\", \"price\"],\n",
        "    expected_dtypes={\n",
        "        \"customer_id\": \"int64\",\n",
        "        \"order_id\": \"int64\",\n",
        "        \"price\": \"float64\",\n",
        "    },\n",
        "    numeric_columns=[\"price\"],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "V0nZg9WVw4I5",
        "outputId": "6fdff8d6-c512-46e4-805b-4834edf6f6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW LLM REPORT (preview) ===\n",
            "\n",
            "OK. I will analyze the validation results and provide a comprehensive report, including issues, root causes, code suggestions, and a final pipeline health report in JSON format.\n",
            "\n",
            "```json\n",
            "{\n",
            " \"issues_detected\": [\n",
            "  \"High percentage of null values in the 'price' column (92%).\",\n",
            "  \"Errors found in logs: 'ERROR', 'WARNING'.\",\n",
            "  \"Stacktrace detected in logs.\",\n",
            "  \"Log classification identified categories: SQL_Error, Python_Error, API_Error, Timeout_Error.\",\n",
            "  \"API failures detected with status code 500.\",\n",
            "  \"API timeouts detected.\",\n",
            "  \"SQL failures detected: 'SQL Error: column age cannot be cast to INT'.\"\n",
            " ],\n",
            " \"root_causes\": [\n",
            "  \"Data quality issue: Missing or unavailable 'price' data leading to a high number of nulls. Potentially due to upstream data errors or incorrect data mapping.\",\n",
            "  \"Applic ...\n",
            "\n",
            "=== STRUCTURED HEALTH REPORT ===\n",
            "\n",
            "=== PIPELINE HEALTH REPORT ===\n",
            "\n",
            "Final Status: UNHEALTHY\n",
            "\n",
            "Issues Detected:\n",
            "  - High percentage of null values in the 'price' column (92%).\n",
            "  - Errors found in logs: 'ERROR', 'WARNING'.\n",
            "  - Stacktrace detected in logs.\n",
            "  - Log classification identified categories: SQL_Error, Python_Error, API_Error, Timeout_Error.\n",
            "  - API failures detected with status code 500.\n",
            "  - API timeouts detected.\n",
            "  - SQL failures detected: 'SQL Error: column age cannot be cast to INT'.\n",
            "\n",
            "Root Causes:\n",
            "  - Data quality issue: Missing or unavailable 'price' data leading to a high number of nulls. Potentially due to upstream data errors or incorrect data mapping.\n",
            "  - Application-level errors: General error and warning logs indicates unhandled exceptions or known bugs in the data processing logic.\n",
            "  - Network issues or resource constraints: Timeout errors likely stem from unstable network connections or overloaded API servers.\n",
            "  - API endpoint issues: API failures (status 500) suggest problems with the external API service being used for data ingestion, potentially due to server errors, rate limiting, or authentication problems.\n",
            "  - Data type mismatch in SQL operations: SQL error indicates that the 'age' column's data type does not match the expected INT type, possibly due to schema changes or inconsistent data in the source system. This can be a schema drift issue that wasn't caught.\n",
            "\n",
            "Python Fixes:\n",
            "  - Implement retry mechanism with exponential backoff for API calls to handle transient network issues and API rate limiting:\n",
            "  - ```python\n",
            "import requests\n",
            "import time\n",
            "\n",
            "def fetch_data_with_retry(url, max_retries=3, backoff_factor=2):\n",
            "    for attempt in range(max_retries):\n",
            "        try:\n",
            "            response = requests.get(url, timeout=5)\n",
            "            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
            "            return response\n",
            "        except requests.exceptions.RequestException as e:\n",
            "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
            "            if attempt == max_retries - 1:\n",
            "                raise  # Re-raise the exception after the last attempt\n",
            "            time.sleep(backoff_factor ** attempt)\n",
            "\n",
            "# Example usage:\n",
            "url = '/orders/v1'\n",
            "try:\n",
            "    response = fetch_data_with_retry(url)\n",
            "    # Process the successful response\n",
            "except requests.exceptions.RequestException as e:\n",
            "    print(f\"Failed to fetch data after multiple retries: {e}\")\n",
            "```\n",
            "  - Add robust error handling to the `fetch_data` function.  Log all exceptions with relevant details for debugging.\n",
            "  - ```python\n",
            "import logging\n",
            "\n",
            "logger = logging.getLogger(__name__)\n",
            "\n",
            "def fetch_data(url):\n",
            "    try:\n",
            "        response = requests.get(url, timeout=5)\n",
            "        response.raise_for_status()\n",
            "        return response.json()\n",
            "    except requests.exceptions.RequestException as e:\n",
            "        logger.error(f'Error fetching data from {url}: {e}', exc_info=True)\n",
            "        return None  # Or raise the exception, depending on the desired behavior\n",
            "    except Exception as e:\n",
            "        logger.exception(f'Unexpected error in fetch_data: {e}')\n",
            "        return None\n",
            "```\n",
            "  - Implement data imputation for the 'price' column, but only after investigating the root cause of the nulls.  A simple approach is to use the mean or median, but more sophisticated methods might be appropriate depending on the data distribution and domain knowledge.\n",
            "  - ```python\n",
            "import pandas as pd\n",
            "\n",
            "def impute_missing_prices(df):\n",
            "    # Check for empty dataframe\n",
            "    if df.empty:\n",
            "        print(\"DataFrame is empty.\")\n",
            "        return df\n",
            "\n",
            "    # Calculate median price\n",
            "    median_price = df['price'].median()\n",
            "\n",
            "    # Impute missing prices with the median\n",
            "    df['price'] = df['price'].fillna(median_price)\n",
            "\n",
            "    return df\n",
            "\n",
            "#Example Usage\n",
            "df = pd.DataFrame({'customer_id': [1,2,3], 'order_id': [101, 102, 103], 'price': [10.0, None, 15.0]})\n",
            "df = impute_missing_prices(df)\n",
            "print(df)\n",
            "```\n",
            "\n",
            "SQL Fixes:\n",
            "  - Investigate and correct the data type of the 'age' column.  If the column is intended to be an integer, clean the data to remove any non-numeric values and update the column type.\n",
            "  - ```sql\n",
            "-- Identify rows with non-numeric values in the 'age' column\n",
            "SELECT age FROM your_table WHERE NOT age ~ '^\\d+$';\n",
            "\n",
            "--Option 1: Clean invalid values to NULL\n",
            "UPDATE your_table SET age = NULL WHERE NOT age ~ '^\\d+$';\n",
            "\n",
            "--Option 2: Filter out invalid values\n",
            "CREATE TEMPORARY TABLE clean_table AS\n",
            "SELECT * FROM your_table WHERE age ~ '^\\d+$';\n",
            "\n",
            "-- Change the data type of the 'age' column to INT (if not already)\n",
            "ALTER TABLE your_table ALTER COLUMN age TYPE INT USING (age::integer);\n",
            "```\n",
            "\n",
            "Recommended Tests:\n",
            "  - Add unit tests to validate the data imputation logic.\n",
            "  - Implement integration tests to verify the API calls and data ingestion process.\n",
            "  - Implement schema validation checks before and after data transformations to detect schema drift.\n",
            "  - Create alerting for null spikes in critical columns.\n",
            "  - Regularly review application logs for errors and warnings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß™ Demo 3 ‚Äî Dtype Mismatch (Strings Inside Numeric Column)\n",
        "\n",
        "**Scenario**\n",
        "\n",
        "The `dtype_mismatch` dataset simulates a very common issue:\n",
        "\n",
        "- Expected:\n",
        "  - `customer_id`: integer\n",
        "  - `age`: integer\n",
        "  - `country`: string\n",
        "- Actual:\n",
        "  - `age` column contains a mix of numeric values and strings (e.g. `\"25\"`, `30`, `\"unknown\"`, `\"fifty\"`)\n",
        "\n",
        "This often leads to **ETL failures** when loading into strongly-typed warehouses or when doing numeric aggregations.\n",
        "\n",
        "**What the agent does**\n",
        "\n",
        "1. The **Dtype Mismatch checker** compares actual `dtypes` against the expected spec and flags:\n",
        "   - `age` as `object` instead of `int64`\n",
        "2. The **Invalid Numeric** check finds rows where `age` cannot be safely cast to a number\n",
        "3. The **LLM agent**:\n",
        "   - Explains why this typically happens (loosely typed upstream systems, manual entry, CSV imports)\n",
        "   - Suggests Python cleansing logic using:\n",
        "     - Regex checks\n",
        "     - Safe casting\n",
        "     - Default values or nulls where parsing fails\n",
        "   - Suggests SQL patterns such as:\n",
        "     - `TRY_CAST(age AS INT)` or\n",
        "     - Using `REGEXP_LIKE` to separate clean vs dirty values\n",
        "   - Recommends using a **staging table** with `TEXT` columns before loading into the final typed table\n",
        "   - Marks the pipeline `unhealthy` until cleansing is added\n",
        "\n",
        "This demo illustrates how the agent bridges **schema expectations** and **actual data content**, and produces concrete cleansing strategies.\n"
      ],
      "metadata": {
        "id": "ppFu1ZfQ3HHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dtype Mismatch (Strings in Age Column)\n",
        "import pandas as pd\n",
        "\n",
        "# ==== Choose ONE ====\n",
        "# Google Drive:\n",
        "df_dtype = pd.read_csv(f\"{project_path}/dtype_mismatch.csv\")\n",
        "with open(f\"{project_path}/pipeline_logs.txt\") as f:\n",
        "    log_text = f.read()\n",
        "\n",
        "# Or local:\n",
        "# df_dtype = pd.read_csv(\"synthetic_data/dtype_mismatch.csv\")\n",
        "# with open(\"synthetic_data/pipeline_logs.txt\") as f:\n",
        "#     log_text = f.read()\n",
        "\n",
        "diagnostics_dtype, report_dtype, health_dtype = diagnose_pipeline(\n",
        "    df=df_dtype,\n",
        "    log_text=log_text,\n",
        "    expected_columns=[\"customer_id\", \"age\", \"country\"],\n",
        "    expected_dtypes={\n",
        "        \"customer_id\": \"int64\",\n",
        "        \"age\": \"int64\",       # we EXPECT int, but actual will be object ‚Üí mismatch\n",
        "        \"country\": \"object\",\n",
        "    },\n",
        "    numeric_columns=[\"age\"],\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "aBgzv61XxyBn",
        "outputId": "9e7c7a95-d4da-487f-cc1b-8aaeffb11ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW LLM REPORT (preview) ===\n",
            "\n",
            "Okay, I've reviewed the validation results. It looks like we have a multi-faceted problem with our data pipeline. Here's a breakdown:\n",
            "\n",
            "```json\n",
            "{\n",
            " \"issues_detected\": [\n",
            "  \"Errors found in logs\",\n",
            "  \"Stacktraces present in logs indicating failures\",\n",
            "  \"Log classification identified SQL, Python, API, and Timeout Errors\",\n",
            "  \"API failures detected with status code 500\",\n",
            "  \"SQL failures due to data type mismatch (age cannot be cast to INT)\",\n",
            "  \"API requests are timing out\"\n",
            " ],\n",
            " \"root_causes\": [\n",
            "  \"Inconsistent data types being loaded into the SQL database.  The 'age' column likely contains values that cannot be converted to integers (e.g., strings, floats, nulls).\",\n",
            "  \"API endpoint /orders/v1 is failing with a 500 status code.  This indicates a server-side error on the API.\",\n",
            "  \"API requests are t ...\n",
            "\n",
            "=== STRUCTURED HEALTH REPORT ===\n",
            "\n",
            "=== PIPELINE HEALTH REPORT ===\n",
            "\n",
            "Final Status: UNHEALTHY\n",
            "\n",
            "Issues Detected:\n",
            "  - Errors found in logs\n",
            "  - Stacktraces present in logs indicating failures\n",
            "  - Log classification identified SQL, Python, API, and Timeout Errors\n",
            "  - API failures detected with status code 500\n",
            "  - SQL failures due to data type mismatch (age cannot be cast to INT)\n",
            "  - API requests are timing out\n",
            "\n",
            "Root Causes:\n",
            "  - Inconsistent data types being loaded into the SQL database.  The 'age' column likely contains values that cannot be converted to integers (e.g., strings, floats, nulls).\n",
            "  - API endpoint /orders/v1 is failing with a 500 status code.  This indicates a server-side error on the API.\n",
            "  - API requests are timing out, likely due to network issues, API server overload, or an insufficient timeout value in the pipeline code.\n",
            "  - Underlying cause of errors may be data quality issues, infrastructure problems, or code defects\n",
            "\n",
            "Python Fixes:\n",
            "  - Implement retry logic with exponential backoff for API requests to handle transient network issues or API server overloads.\n",
            "  - Add try-except blocks around API calls to gracefully handle exceptions and log errors.\n",
            "  - Implement more robust data validation and cleaning steps *before* attempting to load data into the SQL database. This should include type checking and handling of null/invalid values for the 'age' column.\n",
            "  - Adjust the timeout value for API requests (increase if necessary) after monitoring the typical response times.\n",
            "  - Implement logging to capture granular details of errors for debugging\n",
            "\n",
            "SQL Fixes:\n",
            "  - Examine the source data for the 'age' column. Identify and correct any non-integer values.  If nulls are present, decide on a strategy (e.g., replace with a default value, exclude the row, or use a nullable integer type).\n",
            "  - Consider changing the data type of the 'age' column in the SQL database to a type that can accommodate the existing data (e.g., VARCHAR, FLOAT) if the non-integer values are legitimate and need to be stored.\n",
            "  - Alternatively, CAST the 'age' column appropriately during the data load, handling potential conversion errors (e.g. using TRY_CAST in SQL Server or SAFE_CAST in BigQuery).\n",
            "  - Implement SQL error handling to gracefully catch and log any errors that occur during data loading.\n",
            "\n",
            "Recommended Tests:\n",
            "  - **Unit Tests:** Test individual functions (e.g., API request function, data cleaning function) with various inputs, including edge cases and expected error conditions.\n",
            "  - **Integration Tests:** Test the end-to-end data pipeline flow, including API calls, data transformations, and SQL database loading.\n",
            "  - **Data Quality Tests:** Implement automated data quality checks to validate the data before and after each stage of the pipeline.  These tests should include schema validation, data type validation, null value checks, and data completeness checks.\n",
            "  - **API Monitoring:** Monitor the performance and availability of the API endpoints used by the pipeline.  Track response times, error rates, and uptime.\n",
            "  - **Logging and Monitoring:** Implement comprehensive logging throughout the pipeline to track errors, warnings, and performance metrics.  Use a monitoring tool to visualize these logs and set up alerts for critical issues.\n",
            "  - **Timeout Threshold Tests:** Implement alerts for when API response times exceed preset thresholds to identify potential performance regressions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß™ Demo 4 ‚Äî Transformation Failures (Dates, Divide-by-Zero, Totals)\n",
        "\n",
        "**Scenario**\n",
        "\n",
        "The `transformation_errors` dataset focuses on issues that appear **after** the raw data is ingested:\n",
        "\n",
        "- Column `date` contains a mix of valid `YYYY-MM-DD` strings and invalid values like `2025/13/45`\n",
        "- Column `quantity` contains zeros, which can trigger divide-by-zero in downstream KPIs\n",
        "- Column `total` should equal `quantity * unit_price`, but some rows violate this rule\n",
        "\n",
        "These problems often cause runtime errors, incorrect revenue calculations, or silent data corruption.\n",
        "\n",
        "**What the agent does**\n",
        "\n",
        "1. The **Date Parsing checker** tries to parse the `date` column with `%Y-%m-%d` and records all invalid values\n",
        "2. The **Divide-by-Zero checker** identifies rows where `quantity == 0`\n",
        "3. The **Transformation Validation checker** compares `total` against `quantity * unit_price` for each row and flags mismatches\n",
        "4. The **LLM agent**:\n",
        "   - Summarizes the invalid dates, risky divide-by-zero rows, and incorrect totals\n",
        "   - Explains typical causes (inconsistent upstream formatting, business rules not enforced, manual corrections, rounding issues)\n",
        "   - Suggests Python fixes (robust parsing with fallbacks, filtering/flagging bad rows, recomputing totals)\n",
        "   - Suggests SQL fixes (CASE expressions, validation queries, computed columns)\n",
        "   - Recommends regression tests around date parsing and business rule validations\n",
        "   - Marks the pipeline as `unhealthy` until these checks and corrections are built into the transformations\n",
        "\n",
        "This demo shows how the agent can reason about **business logic** and not just low-level data types.\n"
      ],
      "metadata": {
        "id": "TFxQt_Sn3LCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation Errors (Dates + Divide-by-zero + Totals)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==== Choose ONE ====\n",
        "# Google Drive:\n",
        "df_trans = pd.read_csv(f\"{project_path}/transformation_errors.csv\")\n",
        "with open(f\"{project_path}/pipeline_logs.txt\") as f:\n",
        "    log_text = f.read()\n",
        "\n",
        "# Or local:\n",
        "# df_trans = pd.read_csv(\"synthetic_data/transformation_errors.csv\")\n",
        "# with open(\"synthetic_data/pipeline_logs.txt\") as f:\n",
        "#     log_text = f.read()\n",
        "\n",
        "# Define transformation rule: total should be quantity * unit_price\n",
        "rules = {\n",
        "    \"column\": \"total\",\n",
        "    \"logic\": lambda row: (row[\"quantity\"] * row[\"unit_price\"]) if row[\"quantity\"] != 0 else np.nan\n",
        "}\n",
        "\n",
        "diagnostics_trans, report_trans, health_trans = diagnose_pipeline(\n",
        "    df=df_trans,\n",
        "    log_text=log_text,\n",
        "    expected_columns=[\"transaction_id\", \"date\", \"quantity\", \"unit_price\", \"total\"],\n",
        "    expected_dtypes={\n",
        "        \"transaction_id\": \"int64\",\n",
        "        \"date\": \"object\",          # raw string; date_parsing tool will check validity\n",
        "        \"quantity\": \"int64\",\n",
        "        \"unit_price\": \"float64\",\n",
        "        \"total\": \"float64\",\n",
        "    },\n",
        "    numeric_columns=[\"quantity\", \"unit_price\", \"total\"],\n",
        "    date_column=\"date\",\n",
        "    date_format=\"%Y-%m-%d\",        # most rows follow this, invalid ones will be flagged\n",
        "    numerator_col=\"unit_price\",    # just to simulate potential divide-by-zero\n",
        "    denominator_col=\"quantity\",\n",
        "    transformation_rules=rules,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "collapsed": true,
        "id": "tYEH4CJ_yKY6",
        "outputId": "fcdd0419-631f-4761-9cf3-a815d802f935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW LLM REPORT (preview) ===\n",
            "\n",
            "Okay, I've reviewed the diagnostics.  Here's a breakdown of the issues, root causes, and suggested fixes.\n",
            "\n",
            "**1. Issues Detected:**\n",
            "\n",
            "*   **Log Errors:** The logs contain `ERROR` and `WARNING` entries, indicating problems during pipeline execution.\n",
            "*   **Stacktraces:** Stacktraces are present, pinpointing the location of errors in the code.\n",
            "*   **Log Classification:**  Logs are categorized into `SQL_Error`, `Python_Error`, `API_Error`, and `Timeout_Error`, showing diverse failure types.\n",
            "*   **API Failures:** API requests are failing with a 500 status code, and timeout errors are occurring.\n",
            "*   **SQL Failures:** SQL errors are happening due to incompatible data types (e.g., trying to cast `age` to `INT`).\n",
            "*   **Invalid Date Format:** The `date` column contains invalid date values in the forma ...\n",
            "\n",
            "=== STRUCTURED HEALTH REPORT ===\n",
            "\n",
            "=== PIPELINE HEALTH REPORT ===\n",
            "\n",
            "Final Status: UNHEALTHY\n",
            "\n",
            "Issues Detected:\n",
            "  - Log Errors: Logs contain ERROR and WARNING entries.\n",
            "  - Stacktraces: Stacktraces are present in the logs.\n",
            "  - Log Classification: Logs are categorized into SQL_Error, Python_Error, API_Error, and Timeout_Error.\n",
            "  - API Failures: API requests are failing with a 500 status code and timeouts.\n",
            "  - SQL Failures: SQL errors due to incompatible data types (e.g., casting age to INT).\n",
            "  - Invalid Date Format: The 'date' column contains invalid date values.\n",
            "  - Divide by Zero: Division by zero errors occurring during calculation.\n",
            "\n",
            "Root Causes:\n",
            "  - API Failures: Server-side issues with the API (500 error) and network latency (timeouts).\n",
            "  - SQL Failures: Data being inserted into SQL has incompatible data types with table schema.\n",
            "  - Invalid Date Format: Source system or earlier transformation producing incorrect date formats.\n",
            "  - Divide by Zero: Source data contains zero values in the denominator column.\n",
            "\n",
            "Python Fixes:\n",
            "  - fix_date_format(df, date_column='date'): Function to correct invalid date formats.\n",
            "  - handle_divide_by_zero(df, numerator_column, denominator_column): Function to handle divide-by-zero errors.\n",
            "  - fetch_data_with_retry(url): Function to retry API requests with exponential backoff.\n",
            "  - convert_to_int(df, column): Function to safely convert a column to integer type.\n",
            "\n",
            "SQL Fixes:\n",
            "  - SQL: Validate and correct date format using UPDATE with CASE statement.\n",
            "  - SQL: Prevent divide-by-zero errors during update with CASE statement.\n",
            "  - SQL: Perform data type conversions with error handling by using CASE statement.\n",
            "\n",
            "Recommended Tests:\n",
            "  - Data Type Validation: Verify columns have the expected data types.\n",
            "  - Date Format Validation: Ensure dates adhere to a consistent format.\n",
            "  - Range Checks: Confirm that numeric values fall within acceptable ranges.\n",
            "  - Null Value Checks: Monitor null value percentages.\n",
            "  - API Monitoring: Monitor API response times and error rates.\n",
            "  - Database Schema Validation: Ensure data being loaded matches the database schema.\n",
            "  - Exception Handling: Improve exception handling in Python code.\n",
            "  - Unit Tests: Write unit tests for each function\n",
            "  - Integration Tests: Deploy to a test environment and run data to validate the ETL process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Conclusion\n",
        "\n",
        "In this project, I built a **Data Pipeline Doctor Agent** that combines:\n",
        "\n",
        "- **Rule-based validators** for schema, nulls, dtypes, logs and transformations  \n",
        "- A **reasoning-capable LLM** that acts like a senior data engineer  \n",
        "- A unified **Pipeline Health Report** that summarizes issues, root causes and concrete Python/SQL fixes  \n",
        "\n",
        "Across four synthetic but realistic scenarios:\n",
        "\n",
        "1. **Schema Drift & Missing Columns**  \n",
        "2. **Null Spike in a Critical Column**  \n",
        "3. **Dtype Mismatch in a Numeric Field**  \n",
        "4. **Transformation Errors (dates, divide-by-zero, totals)**  \n",
        "\n",
        "the agent was able to:\n",
        "\n",
        "- Detect concrete technical symptoms (missing columns, invalid values, bad dates, etc.)  \n",
        "- Correlate them with log messages (API failures, SQL errors, timeouts)  \n",
        "- Explain likely root causes in plain language  \n",
        "- Propose actionable code snippets and data quality tests  \n",
        "- Output a structured view of pipeline health (`healthy` vs `unhealthy`)  \n",
        "\n",
        "This demonstrates how AI agents can sit **on top of existing data platforms** and provide an automated ‚Äúfirst line of triage‚Äù for data pipeline issues.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Future Work\n",
        "\n",
        "There are several natural extensions to turn this into a production-grade system:\n",
        "\n",
        "- **Tighter Orchestration Integration**  \n",
        "  - Wrap the agent as a task in Airflow / Dagster / Prefect / Azure Data Factory / AWS Glue  \n",
        "  - Trigger it automatically on pipeline failures or on data arrival\n",
        "\n",
        "- **Automatic Fix Execution (Not Just Suggestions)**  \n",
        "  - Move from ‚Äúproposed fixes‚Äù to **approved + applied fixes** with human-in-the-loop  \n",
        "  - Store fix suggestions and approvals in a change log / audit table\n",
        "\n",
        "- **Alerting & Observability**  \n",
        "  - Send the Pipeline Health Report to Slack / Teams / email  \n",
        "  - Expose metrics (number of issues, severity, health status) in a monitoring dashboard\n",
        "\n",
        "- **More Validators and Domain-Specific Rules**  \n",
        "  - Add checks for referential integrity (foreign-key breaks)  \n",
        "  - Add business-level rules (e.g. negative quantities, unrealistic ages, impossible dates)  \n",
        "  - Plug into existing DQ frameworks (Great Expectations, Soda, Deequ, etc.)\n",
        "\n",
        "- **Learning from History**  \n",
        "  - Track recurring issues over time and let the agent propose **preventive changes**  \n",
        "  - Learn which fixes are most effective and prioritize similar actions in the future  \n",
        "\n",
        "Overall, this capstone shows how combining **traditional data validation** with a **tool-using AI agent** can make data platforms more resilient, self-diagnosing, and easier to operate at scale.\n"
      ],
      "metadata": {
        "id": "S94ee-Xt3RhS"
      }
    }
  ]
}
